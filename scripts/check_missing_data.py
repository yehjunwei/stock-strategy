#!/usr/bin/env python3
"""
æª¢æŸ¥ä¸¦è£œé½Šè‡ºè‚¡è³‡æ–™ä¸­ç¼ºå¤±çš„è³‡æ–™
- åˆ†æç¾æœ‰CSVæª”æ¡ˆï¼Œæ‰¾å‡ºç¼ºå¤±çš„æ—¥æœŸå’Œè‚¡ç¥¨
- æä¾›è£œé½Šç¼ºå¤±è³‡æ–™çš„åŠŸèƒ½
"""

import sys
from pathlib import Path
import pandas as pd
from datetime import datetime, timedelta
import argparse
import os

# å˜—è©¦è¼‰å…¥ python-dotenv
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    env_file = Path(__file__).parent.parent / '.env'
    if env_file.exists():
        with open(env_file) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ.setdefault(key, value)

# æ·»åŠ çˆ¶ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.stock_fetcher import TaiwanStockFetcher
from core.line_sender import send_line_message


def get_trading_days(start_date, end_date):
    """
    ç²å–å°è‚¡äº¤æ˜“æ—¥ï¼ˆæ’é™¤é€±æœ«ï¼‰
    æ³¨æ„ï¼šé€™è£¡åªæ’é™¤é€±æœ«ï¼Œä¸åŒ…å«åœ‹å®šå‡æ—¥

    Args:
        start_date: é–‹å§‹æ—¥æœŸ (datetime)
        end_date: çµæŸæ—¥æœŸ (datetime)

    Returns:
        list: äº¤æ˜“æ—¥åˆ—è¡¨
    """
    trading_days = []
    current = start_date

    while current <= end_date:
        # æ’é™¤é€±å…­(5)å’Œé€±æ—¥(6)
        if current.weekday() < 5:
            trading_days.append(current.strftime('%Y-%m-%d'))
        current += timedelta(days=1)

    return trading_days


def analyze_missing_data(csv_path, output_dir):
    """
    åˆ†æCSVä¸­ç¼ºå¤±çš„è³‡æ–™

    Args:
        csv_path: CSVæª”æ¡ˆè·¯å¾‘
        output_dir: è¼¸å‡ºç›®éŒ„

    Returns:
        tuple: (ç¼ºå¤±è³‡æ–™å­—å…¸, çµ±è¨ˆè³‡è¨Š)
    """
    print(f"\n{'='*70}")
    print("ğŸ“Š åˆ†æè³‡æ–™å®Œæ•´æ€§")
    print(f"{'='*70}\n")

    if not csv_path.exists():
        print("âŒ CSVæª”æ¡ˆä¸å­˜åœ¨")
        return {}, {}

    print("ğŸ“‚ è®€å–è³‡æ–™...")
    df = pd.read_csv(csv_path, dtype={'stock_id': str})

    if df.empty:
        print("âŒ CSVæª”æ¡ˆç‚ºç©º")
        return {}, {}

    # åŸºæœ¬çµ±è¨ˆ
    total_records = len(df)
    unique_stocks = df['stock_id'].nunique()
    date_range_start = df['date'].min()
    date_range_end = df['date'].max()

    print(f"âœ“ è³‡æ–™è¼‰å…¥å®Œæˆ")
    print(f"   ç¸½è¨˜éŒ„æ•¸: {total_records:,}")
    print(f"   è‚¡ç¥¨æ•¸é‡: {unique_stocks}")
    print(f"   æ—¥æœŸç¯„åœ: {date_range_start} ~ {date_range_end}")

    # è¨ˆç®—äº¤æ˜“æ—¥
    start_dt = datetime.strptime(date_range_start, '%Y-%m-%d')
    end_dt = datetime.strptime(date_range_end, '%Y-%m-%d')
    all_trading_days = get_trading_days(start_dt, end_dt)
    total_trading_days = len(all_trading_days)

    print(f"\nğŸ“… é æœŸäº¤æ˜“æ—¥æ•¸ï¼ˆæ’é™¤é€±æœ«ï¼‰: {total_trading_days} å¤©")
    print(f"ğŸ” é–‹å§‹æª¢æŸ¥æ¯æ”¯è‚¡ç¥¨çš„è³‡æ–™å®Œæ•´æ€§...\n")

    # åˆ†ææ¯æ”¯è‚¡ç¥¨çš„ç¼ºå¤±æ—¥æœŸ
    missing_data = {}
    stock_stats = []

    all_stocks = sorted(df['stock_id'].unique())

    for idx, stock_id in enumerate(all_stocks, 1):
        stock_df = df[df['stock_id'] == stock_id]
        stock_dates = set(stock_df['date'].tolist())

        # æ‰¾å‡ºç¼ºå¤±çš„æ—¥æœŸ
        missing_dates = [d for d in all_trading_days if d not in stock_dates]

        if missing_dates:
            missing_data[stock_id] = missing_dates

        # çµ±è¨ˆ
        actual_days = len(stock_dates)
        missing_count = len(missing_dates)
        completeness = (actual_days / total_trading_days) * 100 if total_trading_days > 0 else 0

        stock_stats.append({
            'stock_id': stock_id,
            'actual_days': actual_days,
            'missing_days': missing_count,
            'completeness': completeness
        })

        # æ¯100æ”¯è‚¡ç¥¨é¡¯ç¤ºä¸€æ¬¡é€²åº¦
        if idx % 100 == 0:
            print(f"   è™•ç†é€²åº¦: {idx}/{unique_stocks}")

    print(f"\nâœ“ åˆ†æå®Œæˆ\n")

    # é¡¯ç¤ºçµ±è¨ˆçµæœ
    stats_df = pd.DataFrame(stock_stats)

    # å®Œæ•´åº¦çµ±è¨ˆ
    complete_stocks = len(stats_df[stats_df['missing_days'] == 0])
    incomplete_stocks = len(stats_df[stats_df['missing_days'] > 0])

    print(f"{'='*70}")
    print(f"ğŸ“ˆ çµ±è¨ˆæ‘˜è¦")
    print(f"{'='*70}")
    print(f"å®Œæ•´è‚¡ç¥¨æ•¸é‡: {complete_stocks} ({complete_stocks/unique_stocks*100:.1f}%)")
    print(f"ç¼ºå¤±è‚¡ç¥¨æ•¸é‡: {incomplete_stocks} ({incomplete_stocks/unique_stocks*100:.1f}%)")

    if incomplete_stocks > 0:
        total_missing = stats_df['missing_days'].sum()
        avg_missing = stats_df[stats_df['missing_days'] > 0]['missing_days'].mean()
        max_missing = stats_df['missing_days'].max()
        max_missing_stock = stats_df[stats_df['missing_days'] == max_missing].iloc[0]['stock_id']

        print(f"\nç¼ºå¤±è³‡æ–™çµ±è¨ˆ:")
        print(f"   ç¸½ç¼ºå¤±ç­†æ•¸: {total_missing:,} ç­†")
        print(f"   å¹³å‡ç¼ºå¤±å¤©æ•¸: {avg_missing:.1f} å¤©")
        print(f"   æœ€å¤šç¼ºå¤±å¤©æ•¸: {max_missing} å¤© (è‚¡ç¥¨ä»£è™Ÿ: {max_missing_stock})")

        # é¡¯ç¤ºç¼ºå¤±æœ€åš´é‡çš„å‰10æ”¯è‚¡ç¥¨
        print(f"\nç¼ºå¤±æœ€åš´é‡çš„å‰10æ”¯è‚¡ç¥¨:")
        top_10 = stats_df.nlargest(10, 'missing_days')[['stock_id', 'missing_days', 'completeness']]
        for _, row in top_10.iterrows():
            print(f"   {row['stock_id']}: ç¼º {row['missing_days']} å¤© (å®Œæ•´åº¦: {row['completeness']:.1f}%)")

    print(f"{'='*70}\n")

    # å„²å­˜è©³ç´°å ±å‘Š
    report_path = output_dir / "missing_data_report.csv"
    stats_df.to_csv(report_path, index=False, encoding='utf-8-sig')
    print(f"âœ“ è©³ç´°å ±å‘Šå·²å„²å­˜è‡³: {report_path}\n")

    # å„²å­˜ç¼ºå¤±æ˜ç´°
    if missing_data:
        detail_path = output_dir / "missing_data_detail.txt"
        with open(detail_path, 'w', encoding='utf-8') as f:
            f.write(f"ç¼ºå¤±è³‡æ–™æ˜ç´°å ±å‘Š\n")
            f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"{'='*70}\n\n")

            for stock_id in sorted(missing_data.keys()):
                dates = missing_data[stock_id]
                f.write(f"{stock_id}: ç¼º {len(dates)} å¤©\n")
                # é¡¯ç¤ºå‰5å€‹ç¼ºå¤±æ—¥æœŸ
                sample_dates = dates[:5]
                f.write(f"   ç¯„ä¾‹: {', '.join(sample_dates)}")
                if len(dates) > 5:
                    f.write(f" ... (é‚„æœ‰ {len(dates)-5} å¤©)")
                f.write("\n\n")

        print(f"âœ“ ç¼ºå¤±æ˜ç´°å·²å„²å­˜è‡³: {detail_path}\n")

    return missing_data, {
        'total_missing': total_missing if incomplete_stocks > 0 else 0,
        'incomplete_stocks': incomplete_stocks
    }


def fill_missing_data(missing_data, fetcher, max_stocks=None):
    """
    è£œé½Šç¼ºå¤±çš„è³‡æ–™

    Args:
        missing_data: ç¼ºå¤±è³‡æ–™å­—å…¸ {stock_id: [missing_dates]}
        fetcher: TaiwanStockFetcher å¯¦ä¾‹
        max_stocks: æœ€å¤šè£œé½Šå¹¾æ”¯è‚¡ç¥¨ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰
    """
    if not missing_data:
        print("âœ“ æ²’æœ‰ç¼ºå¤±çš„è³‡æ–™éœ€è¦è£œé½Š")
        return 0

    print(f"\n{'='*70}")
    print(f"ğŸ”§ é–‹å§‹è£œé½Šç¼ºå¤±è³‡æ–™")
    print(f"{'='*70}\n")

    total_stocks = len(missing_data)
    if max_stocks:
        print(f"âš ï¸  é™åˆ¶è£œé½Šæ•¸é‡: æœ€å¤š {max_stocks} æ”¯è‚¡ç¥¨\n")
        stocks_to_fill = list(missing_data.keys())[:max_stocks]
    else:
        stocks_to_fill = list(missing_data.keys())

    all_new_data = []
    success_count = 0
    fail_count = 0

    for idx, stock_id in enumerate(stocks_to_fill, 1):
        missing_dates = missing_data[stock_id]

        print(f"[{idx}/{len(stocks_to_fill)}] {stock_id} (ç¼º {len(missing_dates)} å¤©)", end=' ')

        # å°‡é€£çºŒçš„æ—¥æœŸåˆä½µæˆå€é–“ä¾†æ¸›å°‘APIè«‹æ±‚
        date_ranges = _merge_date_ranges(missing_dates)

        stock_data = []
        for start_date, end_date in date_ranges:
            df = fetcher.fetch_stock_data(stock_id, start_date, end_date)
            if df is not None and not df.empty:
                stock_data.append(df)

        if stock_data:
            combined = pd.concat(stock_data, ignore_index=True)
            all_new_data.append(combined)
            success_count += 1
            print(f"âœ“ è£œé½Š {len(combined)} ç­†")
        else:
            fail_count += 1
            print("âœ— å¤±æ•—")

    print(f"\n{'='*70}")
    print(f"è£œé½Šçµæœ:")
    print(f"   æˆåŠŸ: {success_count}/{len(stocks_to_fill)}")
    print(f"   å¤±æ•—: {fail_count}/{len(stocks_to_fill)}")
    print(f"{'='*70}\n")

    # åˆä½µä¸¦å„²å­˜
    if all_new_data:
        final_df = pd.concat(all_new_data, ignore_index=True)
        print(f"ğŸ’¾ å„²å­˜è£œé½Šçš„è³‡æ–™...")
        fetcher.merge_and_save(final_df)
        return len(final_df)
    else:
        print("âš ï¸  æ²’æœ‰æˆåŠŸè£œé½Šä»»ä½•è³‡æ–™")
        return 0


def _merge_date_ranges(dates):
    """
    å°‡æ—¥æœŸåˆ—è¡¨åˆä½µæˆé€£çºŒçš„æ—¥æœŸå€é–“

    Args:
        dates: æ—¥æœŸå­—ä¸²åˆ—è¡¨ ['2024-01-01', '2024-01-02', ...]

    Returns:
        list: [(start_date, end_date), ...]
    """
    if not dates:
        return []

    # è½‰æ›ç‚º datetime ä¸¦æ’åº
    date_objs = sorted([datetime.strptime(d, '%Y-%m-%d') for d in dates])

    ranges = []
    start = date_objs[0]
    end = date_objs[0]

    for i in range(1, len(date_objs)):
        if (date_objs[i] - end).days <= 3:  # å…è¨±3å¤©å…§çš„é–“éš”ï¼ˆè€ƒæ…®é€±æœ«ï¼‰
            end = date_objs[i]
        else:
            ranges.append((start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d')))
            start = date_objs[i]
            end = date_objs[i]

    ranges.append((start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d')))
    return ranges


def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(description='æª¢æŸ¥ä¸¦è£œé½Šè‡ºè‚¡è³‡æ–™ä¸­ç¼ºå¤±çš„è³‡æ–™')
    parser.add_argument(
        '--check-only',
        action='store_true',
        help='åƒ…æª¢æŸ¥ï¼Œä¸è£œé½Šè³‡æ–™'
    )
    parser.add_argument(
        '--max-stocks',
        type=int,
        help='æœ€å¤šè£œé½Šå¹¾æ”¯è‚¡ç¥¨ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='data',
        help='è³‡æ–™ç›®éŒ„ï¼ˆé è¨­: dataï¼‰'
    )

    args = parser.parse_args()

    print("\n" + "="*70)
    print("ğŸ” è‡ºè‚¡è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥å·¥å…·")
    print("="*70)

    # åˆå§‹åŒ–
    output_dir = Path(args.output_dir)
    csv_path = output_dir / "taiwan_stocks.csv"

    # åˆ†æç¼ºå¤±è³‡æ–™
    missing_data, stats = analyze_missing_data(csv_path, output_dir)

    # å¦‚æœåªæ˜¯æª¢æŸ¥ï¼Œå°±çµæŸ
    if args.check_only:
        print("âœ“ æª¢æŸ¥å®Œæˆï¼ˆåƒ…æª¢æŸ¥æ¨¡å¼ï¼Œæœªè£œé½Šè³‡æ–™ï¼‰\n")
        return

    # è£œé½Šç¼ºå¤±è³‡æ–™
    if missing_data:
        api_token = os.getenv('FINMIND_API_TOKEN')
        fetcher = TaiwanStockFetcher(api_token=api_token, output_dir=args.output_dir)

        filled_count = fill_missing_data(missing_data, fetcher, max_stocks=args.max_stocks)

        print(f"\nâœ… ä»»å‹™å®Œæˆï¼å…±è£œé½Š {filled_count:,} ç­†è³‡æ–™\n")

        # ç™¼é€é€šçŸ¥
        message = (
            f"ã€è³‡æ–™è£œé½Šå ±å‘Šã€‘\n"
            f"- ç¼ºå¤±è‚¡ç¥¨æ•¸: {stats['incomplete_stocks']}\n"
            f"- ç¸½ç¼ºå¤±ç­†æ•¸: {stats['total_missing']:,}\n"
            f"- å·²è£œé½Šç­†æ•¸: {filled_count:,}"
        )
        send_line_message(message)
    else:
        print("âœ… è³‡æ–™å®Œæ•´ï¼Œç„¡éœ€è£œé½Š\n")


if __name__ == "__main__":
    main()
