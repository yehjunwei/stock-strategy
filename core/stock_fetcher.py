"""
è‡ºè‚¡è³‡æ–™ç²å–å™¨æ ¸å¿ƒé¡
æä¾›è‡ºè‚¡æ­·å²è³‡æ–™çš„å¢é‡ç²å–åŠŸèƒ½
"""

import pandas as pd
from datetime import datetime, timedelta
import time
from pathlib import Path
import json

try:
    from FinMind.data import DataLoader
except ImportError:
    print("âŒ è«‹å…ˆå®‰è£ä¾è³´: pip install -r requirements.txt")
    exit(1)


class TaiwanStockFetcher:
    """è‡ºè‚¡è³‡æ–™å¢é‡ç²å–å™¨"""

    TARGET_START_DATE = "2000-01-01"
    CSV_FILENAME = "taiwan_stocks.csv"

    def __init__(self, api_token=None, output_dir="data"):
        """åˆå§‹åŒ–ç²å–å™¨"""
        self.api = DataLoader()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.csv_path = self.output_dir / self.CSV_FILENAME
        self.stock_name_map = {}  # è‚¡ç¥¨ä»£è™Ÿ -> ä¸­æ–‡åç¨±å°æ‡‰

        if api_token:
            self.api.login_by_token(api_token=api_token)
            print("âœ“ å·²ä½¿ç”¨ API Token ç™»å…¥")
        else:
            print("â„¹ï¸  æœªä½¿ç”¨ API Tokenï¼ˆè«‹æ±‚é »ç‡å—é™ï¼‰")

        # å˜—è©¦å¾ç¾æœ‰æª”æ¡ˆè¼‰å…¥è‚¡ç¥¨åç¨±å°æ‡‰
        self._load_stock_name_map()

    def _load_stock_name_map(self):
        """å¾ç¾æœ‰çš„ stock_list.json è¼‰å…¥è‚¡ç¥¨åç¨±å°æ‡‰"""
        json_path = self.output_dir / "stock_list.json"
        if json_path.exists():
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for stock in data.get('stocks', []):
                        self.stock_name_map[stock['stock_id']] = stock['stock_name']
                print(f"âœ“ å·²è¼‰å…¥ {len(self.stock_name_map)} å€‹è‚¡ç¥¨åç¨±å°æ‡‰")
            except Exception as e:
                print(f"âš ï¸  è¼‰å…¥è‚¡ç¥¨åç¨±å°æ‡‰å¤±æ•—: {e}")

    def get_existing_data_info(self):
        """
        ç²å–ç¾æœ‰è³‡æ–™è³‡è¨Š
        è¿”å›: (æ˜¯å¦å­˜åœ¨, æœ€æ—©æ—¥æœŸ, æœ€æ™šæ—¥æœŸ, è¨˜éŒ„æ•¸)
        """
        if not self.csv_path.exists():
            return False, None, None, 0

        try:
            df = pd.read_csv(self.csv_path, usecols=['date'])
            if df.empty:
                return False, None, None, 0

            dates = pd.to_datetime(df['date']).sort_values()
            earliest = dates.min().strftime('%Y-%m-%d')
            latest = dates.max().strftime('%Y-%m-%d')
            count = len(df)
            return True, earliest, latest, count

        except Exception as e:
            print(f"âš ï¸  è®€å–ç¾æœ‰è³‡æ–™å¤±æ•—: {e}")
            return False, None, None, 0

    def calculate_fetch_ranges(self, existing_earliest_date, existing_latest_date):
        """
        è¨ˆç®—éœ€è¦ç²å–çš„æ—¥æœŸç¯„åœï¼ˆå…©éšæ®µï¼‰

        ç­–ç•¥:
        1. ç¬¬ä¸€éšæ®µï¼šå¾æœ€æ–°æ—¥æœŸåˆ°ä»Šå¤©ï¼ˆè£œé½Šæœ€æ–°è³‡æ–™ï¼‰
        2. ç¬¬äºŒéšæ®µï¼šå¾æœ€æ—©æ—¥æœŸå¾€å›æŠ“åˆ° 2000 å¹´ï¼ˆè£œé½Šæ­·å²è³‡æ–™ï¼‰

        è¿”å›: [(start_date, end_date, description), ...]
        """
        ranges = []
        today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        target_date = datetime.strptime(self.TARGET_START_DATE, "%Y-%m-%d")

        if existing_latest_date is None:
            start_date = today - timedelta(days=365)
            ranges.append((
                start_date.strftime('%Y-%m-%d'),
                today.strftime('%Y-%m-%d'),
                "é¦–æ¬¡åŸ·è¡Œï¼ˆæœ€è¿‘1å¹´ï¼‰"
            ))
        else:
            latest_date = datetime.strptime(existing_latest_date, "%Y-%m-%d")
            days_gap = (today - latest_date).days

            if days_gap > 1:
                ranges.append((
                    (latest_date + timedelta(days=1)).strftime('%Y-%m-%d'),
                    today.strftime('%Y-%m-%d'),
                    f"æ›´æ–°æœ€æ–°è³‡æ–™ï¼ˆè£œé½Š {days_gap} å¤©ï¼‰"
                ))

            if existing_earliest_date:
                earliest_date = datetime.strptime(existing_earliest_date, "%Y-%m-%d")

                if earliest_date > target_date:
                    end_date = earliest_date - timedelta(days=1)
                    start_date = end_date - timedelta(days=365)

                    if start_date < target_date:
                        start_date = target_date

                    days_to_fetch = (end_date - start_date).days
                    ranges.append((
                        start_date.strftime('%Y-%m-%d'),
                        end_date.strftime('%Y-%m-%d'),
                        f"è£œå……æ­·å²è³‡æ–™ï¼ˆå¾€å‰ {days_to_fetch} å¤©ï¼‰"
                    ))

        return ranges

    def get_stock_list(self):
        """ç²å–æ‰€æœ‰ä¸Šå¸‚è‚¡ç¥¨åˆ—è¡¨"""
        print("\nğŸ“‹ æ­£åœ¨ç²å–è‡ºè‚¡åˆ—è¡¨...")

        try:
            stock_info = self.api.taiwan_stock_info()

            # é™¤éŒ¯è³‡è¨Š
            print(f"ğŸ” API å›æ‡‰é¡å‹: {type(stock_info)}")
            if stock_info is None:
                print("âš ï¸  API å›æ‡‰ç‚º None")
                return []

            # æª¢æŸ¥æ˜¯å¦ç‚º DataFrame
            if hasattr(stock_info, 'empty'):
                print(f"ğŸ” DataFrame æ˜¯å¦ç‚ºç©º: {stock_info.empty}")
                if not stock_info.empty:
                    print(f"ğŸ” DataFrame æ¬„ä½: {stock_info.columns.tolist()}")
                    print(f"ğŸ” DataFrame è¡Œæ•¸: {len(stock_info)}")

            if stock_info is not None and not stock_info.empty:
                # ç¯©é¸ä¸Šå¸‚è‚¡ç¥¨ï¼ˆ4ä½æ•¸ä»£ç¢¼ï¼‰
                filtered = stock_info[
                    (stock_info['type'] == 'twse') &
                    (stock_info['stock_id'].str.len() == 4)
                ]

                # å»ºç«‹è‚¡ç¥¨ä»£è™Ÿåˆ°åç¨±çš„å°æ‡‰
                self.stock_name_map = dict(
                    zip(filtered['stock_id'], filtered['stock_name'])
                )

                sorted_stocks = sorted(filtered['stock_id'].unique().tolist())
                print(f"âœ“ ç²å–åˆ° {len(sorted_stocks)} æ”¯ä¸Šå¸‚è‚¡ç¥¨")

                # å„²å­˜è‚¡ç¥¨åˆ—è¡¨åˆ°æª”æ¡ˆ
                self._save_stock_list(sorted_stocks)

                return sorted_stocks
            else:
                print("âŒ ç„¡æ³•ç²å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆå›æ‡‰ç‚ºç©ºï¼‰")
                return []

        except KeyError as e:
            print(f"âŒ KeyError: {e}")
            print(f"ğŸ” é€™å¯èƒ½æ˜¯ FinMind API å›æ‡‰æ ¼å¼å•é¡Œ")
            import traceback
            traceback.print_exc()
            return []
        except Exception as e:
            print(f"âŒ ç²å–è‚¡ç¥¨åˆ—è¡¨å¤±æ•—: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _save_stock_list(self, stocks):
        """å„²å­˜è‚¡ç¥¨åˆ—è¡¨åˆ°æª”æ¡ˆ"""
        if not stocks:
            return

        # å„²å­˜ç‚º TXT æª”æ¡ˆï¼ˆè‚¡ç¥¨ä»£è™Ÿ + ä¸­æ–‡åç¨±ï¼‰
        txt_path = self.output_dir / "stock_list.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            for stock_id in stocks:
                stock_name = self.stock_name_map.get(stock_id, '')
                f.write(f"{stock_id}\t{stock_name}\n")

        # å„²å­˜ç‚º CSV æª”æ¡ˆï¼ˆæ–¹ä¾¿ Excel é–‹å•Ÿï¼‰
        csv_path = self.output_dir / "stock_list.csv"
        with open(csv_path, 'w', encoding='utf-8-sig') as f:
            f.write("stock_id,stock_name\n")
            for stock_id in stocks:
                stock_name = self.stock_name_map.get(stock_id, '')
                f.write(f"{stock_id},{stock_name}\n")

        # å„²å­˜ç‚º JSON æª”æ¡ˆï¼ˆåŒ…å«è©³ç´°è³‡è¨Šï¼‰
        json_path = self.output_dir / "stock_list.json"
        stock_list_with_names = [
            {"stock_id": stock_id, "stock_name": self.stock_name_map.get(stock_id, '')}
            for stock_id in stocks
        ]
        stock_info = {
            "update_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "total_count": len(stocks),
            "filter_criteria": {
                "type": "twse",
                "stock_id_length": 4
            },
            "stocks": stock_list_with_names
        }
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(stock_info, f, ensure_ascii=False, indent=2)

        print(f"âœ“ è‚¡ç¥¨åˆ—è¡¨å·²å„²å­˜åˆ°:")
        print(f"   - {txt_path} (Tab å­—å…ƒåˆ†éš”)")
        print(f"   - {csv_path} (CSVæ ¼å¼)")
        print(f"   - {json_path} (JSONæ ¼å¼)")

    def fetch_stock_data(self, stock_id, start_date, end_date):
        """ç²å–å–®ä¸€è‚¡ç¥¨çš„æ­·å²è³‡æ–™"""
        try:
            df = self.api.taiwan_stock_daily(
                stock_id=stock_id,
                start_date=start_date,
                end_date=end_date
            )

            if df is not None and not df.empty:
                # ç²å–è‚¡ç¥¨åç¨±
                stock_name = self.stock_name_map.get(stock_id, '')

                df_clean = pd.DataFrame({
                    'date': pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d'),
                    'stock_id': df['stock_id'],
                    'stock_name': stock_name,
                    'open': df['open'],
                    'high': df['max'],
                    'low': df['min'],
                    'close': df['close'],
                    'volume': df['Trading_Volume']
                })
                return df_clean
            else:
                return None

        except Exception:
            return None

    def fetch_batch(self, stock_list, start_date, end_date, delay=0.5):
        """æ‰¹æ¬¡ç²å–è‚¡ç¥¨è³‡æ–™"""
        print(f"\n{'='*70}")
        print(f"ğŸ“¥ é–‹å§‹ç²å–è³‡æ–™: {start_date} è‡³ {end_date}")
        print(f"{'='*70}\n")

        all_data = []
        total = len(stock_list)
        success_count = 0
        fail_count = 0

        for idx, stock_id in enumerate(stock_list, 1):
            percentage = (idx / total) * 100
            print(f"[{idx}/{total}] ({percentage:.1f}%) {stock_id} ", end='', flush=True)

            df = self.fetch_stock_data(stock_id, start_date, end_date)

            if df is not None and not df.empty:
                all_data.append(df)
                success_count += 1
                print(f"âœ“ {len(df)} æ¢")
            else:
                fail_count += 1
                print("âœ—")

            if idx < total:
                time.sleep(delay)

            if idx % 50 == 0:
                print(f"\n   é€²åº¦çµ±è¨ˆ: æˆåŠŸ {success_count} | å¤±æ•— {fail_count}\n")

        if all_data:
            final_df = pd.concat(all_data, ignore_index=True)
            self._print_batch_summary(final_df, success_count, fail_count, total)
            return final_df
        else:
            print("\nâŒ æœªç²å–åˆ°ä»»ä½•è³‡æ–™")
            return pd.DataFrame()

    def _print_batch_summary(self, df, success_count, fail_count, total):
        """åˆ—å°æ‰¹æ¬¡ç²å–æ‘˜è¦"""
        print(f"\n{'='*70}")
        print(f"âœ“ æœ¬æ¬¡ç²å–å®Œæˆ")
        print(f"  æ–°å¢è¨˜éŒ„: {len(df):,} æ¢")
        print(f"  æˆåŠŸè‚¡ç¥¨: {success_count}/{total}")
        print(f"  å¤±æ•—è‚¡ç¥¨: {fail_count}/{total}")
        print(f"{'='*70}\n")

    def merge_and_save(self, new_df):
        """åˆä½µæ–°èˆŠè³‡æ–™ä¸¦å„²å­˜"""
        if new_df.empty:
            print("âš ï¸  æ²’æœ‰æ–°è³‡æ–™éœ€è¦å„²å­˜")
            return

        if self.csv_path.exists():
            print("ğŸ“‚ æ­£åœ¨è®€å–ç¾æœ‰è³‡æ–™...")
            existing_df = pd.read_csv(self.csv_path, dtype={'stock_id': str})
            print(f"   ç¾æœ‰è¨˜éŒ„: {len(existing_df):,} æ¢")

            # ç‚ºèˆŠè³‡æ–™å¡«å……ç¼ºå¤±çš„ stock_name
            if 'stock_name' not in existing_df.columns:
                existing_df['stock_name'] = ''

            # å¡«å……ç©ºçš„ stock_name
            mask = existing_df['stock_name'].isna() | (existing_df['stock_name'] == '')
            if mask.any():
                existing_df.loc[mask, 'stock_name'] = existing_df.loc[mask, 'stock_id'].map(
                    self.stock_name_map
                ).fillna('')

            print("ğŸ”„ åˆä½µæ–°èˆŠè³‡æ–™...")
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)

            # å¡«å……æ‰€æœ‰ç©ºçš„ stock_name
            mask = combined_df['stock_name'].isna() | (combined_df['stock_name'] == '')
            if mask.any():
                combined_df.loc[mask, 'stock_name'] = combined_df.loc[mask, 'stock_id'].map(
                    self.stock_name_map
                ).fillna('')

            print("ğŸ§¹ å»é™¤é‡è¤‡è¨˜éŒ„...")
            combined_df = combined_df.drop_duplicates(
                subset=['date', 'stock_id'],
                keep='last'
            )
        else:
            print("ğŸ“ å»ºç«‹æ–°è³‡æ–™æª”æ¡ˆ...")
            combined_df = new_df

        print("ğŸ“Š æ’åºè³‡æ–™...")
        combined_df = combined_df.sort_values(['date', 'stock_id']).reset_index(drop=True)

        # ç¢ºä¿æ¬„ä½é †åºæ­£ç¢º
        desired_columns = ['date', 'stock_id', 'stock_name', 'open', 'high', 'low', 'close', 'volume']
        combined_df = combined_df[desired_columns]

        print(f"ğŸ’¾ å„²å­˜åˆ° {self.csv_path}...")
        combined_df.to_csv(self.csv_path, index=False, encoding='utf-8-sig')

        self._print_save_summary(combined_df)

    def _print_save_summary(self, df):
        """åˆ—å°å„²å­˜æ‘˜è¦"""
        file_size_mb = self.csv_path.stat().st_size / 1024 / 1024
        date_range = f"{df['date'].min()} ~ {df['date'].max()}"
        stock_count = df['stock_id'].nunique()

        print(f"\n{'='*70}")
        print(f"âœ… è³‡æ–™å·²å„²å­˜")
        print(f"   æª”æ¡ˆè·¯å¾‘: {self.csv_path}")
        print(f"   æª”æ¡ˆå¤§å°: {file_size_mb:.2f} MB")
        print(f"   ç¸½è¨˜éŒ„æ•¸: {len(df):,} æ¢")
        print(f"   è‚¡ç¥¨æ•¸é‡: {stock_count} æ”¯")
        print(f"   æ—¥æœŸç¯„åœ: {date_range}")
        print(f"{'='*70}\n")

    def show_preview(self, df, n=5):
        """é¡¯ç¤ºè³‡æ–™é è¦½"""
        if df.empty:
            return

        print(f"è³‡æ–™é è¦½ï¼ˆå‰ {n} æ¢ï¼‰:")
        print(df.head(n).to_string(index=False))
        print()
